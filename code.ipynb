{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, random, gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = np.array([['-', '-', '-'],\n",
    "                  ['-', '-', '-'],\n",
    "                  ['-', '-', '-']])             \n",
    "players = ['X', 'O']                             \n",
    "num_players = len(players)\n",
    "Q = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['-', '-', '-'],\n",
       "       ['-', '-', '-'],\n",
       "       ['-', '-', '-']], dtype='<U1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 0.5\n",
    "num_episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  |  -  |  -\n",
      "--------------\n",
      "-  |  -  |  -\n",
      "--------------\n",
      "-  |  -  |  -\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "def print_board(board):\n",
    "    for row in board:\n",
    "        print('  |  '.join(row))\n",
    "        print('--------------')\n",
    "print_board(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Function to convert the board state to a string to use it as a key in the Q-table dictionary.\n",
    "def board_to_string(board):\n",
    "    return ''.join(board.flatten())\n",
    "board_to_string(board)\n",
    "\n",
    "\n",
    "#defining action as a cell randomly selected from the empty cells\n",
    "empty_cells = np.argwhere(board == '-')\n",
    "action = tuple(random.choice(empty_cells))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the game is over by checking different winning condition\n",
    "\n",
    "def is_game_over(board):\n",
    "\n",
    "    # Check rows for winning condition\n",
    "    for row in board:\n",
    "        if len(set(row)) == 1 and row[0] != '-':        #len(set(row)) == 1 -> check if all elements in row are same and  none of the cell is empty\n",
    "            return True, row[0]\n",
    "\n",
    "\n",
    "    # Check columns\n",
    "    for col in board.T:                                 #iterate over clms of transponse of board\n",
    "        if len(set(col)) == 1 and col[0] != '-':\n",
    "            return True, col[0]\n",
    "\n",
    "\n",
    "    # Check diagonals\n",
    "    if len(set(board.diagonal())) == 1 and board[0, 0] != '-':             #check all elements in main diagonal are same and non empty\n",
    "        return True, board[0, 0]\n",
    "    if len(set(np.fliplr(board).diagonal())) == 1 and board[0, 2] != '-':   #horizontal flip the board and check...\n",
    "        return True, board[0, 2]\n",
    "\n",
    "\n",
    "    # Check if the board is full\n",
    "    if '-' not in board:\n",
    "        return True, 'draw'\n",
    "\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose an action based on the Q-table\n",
    "\n",
    "#Random exploration condition in the choose_action function checks whether agent should perform a random exploration or not or if current state is not present in the Q-table\n",
    "#if random exploration is choosen,\n",
    "#a random action is chosen from the available empty cells on the board.\n",
    "# This promotes exploration and allows the agent to try out different actions and gather more information about the environment.\n",
    "\n",
    "\n",
    "#if exploitation is choosen,\n",
    "#the function selects the action with the highest Q-value from the available empty cells.\n",
    "#and do action - > update it with player symbol (X or O according to player[])\n",
    "\n",
    "def choose_action(board, exploration_rate):\n",
    "    state = board_to_string(board)\n",
    "\n",
    "    # Exploration-exploitation trade-off\n",
    "    if random.uniform(0, 1) < exploration_rate or state not in Q:\n",
    "        # Choose a random action\n",
    "        empty_cells = np.argwhere(board == '-')\n",
    "        action = tuple(random.choice(empty_cells))\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        q_values = Q[state]\n",
    "        empty_cells = np.argwhere(board == '-')                                    #returns indices of the empty cells in the board.\n",
    "        empty_q_values = [q_values[cell[0], cell[1]] for cell in empty_cells]      #retrieves Q-values corresponding to each empty cells.\n",
    "        max_q_value = max(empty_q_values)                                          #find the maximum Q-value among the empty cells Qvalue\n",
    "        max_q_indices = [i for i in range(len(empty_cells)) if empty_q_values[i] == max_q_value]    #retrieves the indices of empty cells that have the maximum Q-value.\n",
    "        max_q_index = random.choice(max_q_indices)                                 #if there are multiple cells with same maximum Q value select 1 randomly\n",
    "        action = tuple(empty_cells[max_q_index])                                   #retrieves the indices of the selected empty cell based on max_q_index\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  convert the cell coordinates (row and column) of the chosen action to the next state of the board as a string.\n",
    "\n",
    "def board_next_state(cell):\n",
    "    next_state = board.copy()                      #create a copy of current board state\n",
    "    next_state[cell[0], cell[1]] = players[0]\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  |  -  |  -\n",
      "--------------\n",
      "-  |  -  |  -\n",
      "--------------\n",
      "-  |  -  |  O\n",
      "--------------\n",
      "-  |  -  |  -\n",
      "--------------\n",
      "X  |  O  |  -\n",
      "--------------\n",
      "-  |  -  |  O\n",
      "--------------\n",
      "-  |  O  |  -\n",
      "--------------\n",
      "X  |  O  |  -\n",
      "--------------\n",
      "-  |  -  |  X\n",
      "--------------\n",
      "-  |  X  |  O\n",
      "--------------\n",
      "X  |  O  |  -\n",
      "--------------\n",
      "-  |  -  |  X\n",
      "--------------\n",
      "-  |  X  |  O\n",
      "--------------\n",
      "X  |  X  |  O\n",
      "--------------\n",
      "-  |  -  |  X\n",
      "--------------\n",
      "-  |  X  |  O\n",
      "--------------\n",
      "X  |  X  |  O\n",
      "--------------\n",
      "-  |  X  |  X\n",
      "--------------\n",
      "Human player wins!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to update the Q-table\n",
    "agent_wins = 0\n",
    "# def update_q_table(state, action, next_state, reward):\n",
    "#     q_values = Q.get(state, np.zeros((3, 3)))                               #Retrieve the Q-values for a particular state from the Q-table dictionary Q.\n",
    "#     next_q_values = Q.get(board_to_string(next_state), np.zeros((3, 3)))       # Calculate the maximum Q-value for the next state from q table\n",
    "#     max_next_q_value = np.max(next_q_values)                                #find maxmium q values from q values of nxt state\n",
    "\n",
    "\n",
    "\n",
    "#     # Q-learning update equation\n",
    "#     q_values[action[0], action[1]] += learning_rate * (reward + discount_factor * max_next_q_value - q_values[action[0], action[1]])\n",
    "# #Q-learning update equation calculates the new Q-value for the current state-action pair based on the immediate reward, the discounted future rewards, and the current Q-value.\n",
    "# #By subtracting the current Q-value from the estimated total reward, it calculates the temporal difference (TD) error, which represents the discrepancy between the expected reward and the actual reward.\n",
    "\n",
    "\n",
    "# #The new Q-value is obtained by updating the current Q-value using the TD error, the learning rate, and the discount factor. This update process helps the Q-values to gradually converge towards the optimal values, reflecting the expected long-term rewards for each state-action pair.\n",
    "#     Q[state] = q_values\n",
    "\n",
    "def update_q_table(state, action, next_state, reward):\n",
    "    q_values = Q.get(state, np.zeros((3, 3)))\n",
    "\n",
    "    # Calculate the maximum Q-value for the next state\n",
    "    next_q_values = Q.get(board_to_string(next_state), np.zeros((3, 3)))\n",
    "    max_next_q_value = np.max(next_q_values)\n",
    "\n",
    "    # Q-learning update equation\n",
    "    q_values[action[0], action[1]] += learning_rate * (reward + discount_factor * max_next_q_value - q_values[action[0], action[1]])\n",
    "\n",
    "    Q[state] = q_values\n",
    "\n",
    "# Main Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    board = np.array([['-', '-', '-'],\n",
    "                      ['-', '-', '-'],\n",
    "                      ['-', '-', '-']])\n",
    "\n",
    "    current_player = random.choice(players)\n",
    "    game_over = False\n",
    "\n",
    "    while not game_over:\n",
    "        # Choose an action based on the current state\n",
    "        action = choose_action(board, exploration_rate)\n",
    "\n",
    "        # Make the chosen move\n",
    "        row, col = action\n",
    "        board[row, col] = current_player\n",
    "\n",
    "        # Check if the game is over\n",
    "        game_over, winner = is_game_over(board)\n",
    "\n",
    "        if game_over:\n",
    "            # Update the Q-table with the final reward\n",
    "            if winner == current_player:\n",
    "                reward = 1\n",
    "            elif winner == 'draw':\n",
    "                reward = 0.5\n",
    "            else:\n",
    "                reward = 0\n",
    "            update_q_table(board_to_string(board), action, board, reward)\n",
    "        else:\n",
    "            # Switch to the next player\n",
    "            current_player = players[(players.index(current_player) + 1) % num_players]\n",
    "\n",
    "        # Update the Q-table based on the immediate reward and the next state\n",
    "        if not game_over:\n",
    "            next_state = board_next_state(action)\n",
    "            update_q_table(board_to_string(board), action, next_state, 0)\n",
    "\n",
    "    # Decay the exploration rate\n",
    "    exploration_rate *= 0.99\n",
    "\n",
    "# Play against the trained agent\n",
    "board = np.array([['-', '-', '-'],\n",
    "                  ['-', '-', '-'],\n",
    "                  ['-', '-', '-']])\n",
    "\n",
    "current_player = random.choice(players)\n",
    "game_over = False\n",
    "\n",
    "# ...\n",
    "\n",
    "while not game_over:\n",
    "    if current_player == 'X':\n",
    "        # Human player's turn\n",
    "        print_board(board)\n",
    "        row = int(input(\"Enter the row (0-2): \"))\n",
    "        col = int(input(\"Enter the column (0-2): \"))\n",
    "        action = (row, col)\n",
    "    else:\n",
    "        # Trained agent's turn\n",
    "        action = choose_action(board, exploration_rate=0)\n",
    "\n",
    "    row, col = action\n",
    "    board[row, col] = current_player\n",
    "\n",
    "    game_over, winner = is_game_over(board)\n",
    "\n",
    "    if game_over:\n",
    "        print_board(board)\n",
    "        if winner == 'X':\n",
    "            print(\"Human player wins!\")\n",
    "        elif winner == 'O':\n",
    "            print(\"Agent wins!\")\n",
    "        else:\n",
    "            print(\"It's a draw!\")\n",
    "    else:\n",
    "        current_player = players[(players.index(current_player) + 1) % num_players]\n",
    "\n",
    "#agent_win_percentage = (agent_wins / num_games) * 100\n",
    "#print(\"Agent win percentage: {:.2f}%\".format(agent_win_percentage))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
